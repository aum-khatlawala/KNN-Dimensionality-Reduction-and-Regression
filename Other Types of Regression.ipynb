{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-pepper",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 4\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Linear Regression`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6269f1",
   "metadata": {},
   "source": [
    "Name: Aum Alok Khatlawala <br>\n",
    "Roll Number: 2020113008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-transaction",
   "metadata": {},
   "source": [
    "## Background about the dataset\n",
    "\n",
    "TLDR: You have 4 independent variables (`float`) for each molecule. You can use a linear combination of these 4 independent variables to predict the bandgap (dependent variable) of each molecule.\n",
    "\n",
    "You can read more about the problem in [Li et al, Bandgap tuning strategy by cations and halide ions of lead halide perovskites learned from machine learning, RSC Adv., 2021,11, 15688-15694](https://doi.org/10.1039/D1RA03117A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lyric-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_molecules = list()\n",
    "\n",
    "with open('bg_data.txt', 'r') as infile:\n",
    "    input_rows = csv.DictReader(infile)\n",
    "    \n",
    "    for row in input_rows:\n",
    "        current_mol = ([float(row['Cs']), float(row['FA']), float(row['Cl']), float(row['Br'])], float(row['Bandgap']))\n",
    "        all_molecules.append(current_mol)\n",
    "\n",
    "random.shuffle(all_molecules)\n",
    "\n",
    "\n",
    "num_train = int(len(all_molecules) * 0.8)\n",
    "\n",
    "# each point in x_train has 4 values - 1 for each feature\n",
    "x_train = [x[0] for x in all_molecules[:num_train]]\n",
    "# each point in y_train has 1 value - the bandgap of the molecule\n",
    "y_train = [x[1] for x in all_molecules[:num_train]]\n",
    "\n",
    "x_test = [x[0] for x in all_molecules[num_train:]]\n",
    "y_test = [x[1] for x in all_molecules[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-direction",
   "metadata": {},
   "source": [
    "### 4.1 Implement a Linear Regression model that minimizes the MSE **without using any libraries**. You may use NumPy to vectorize your code, but *do not use numpy.polyfit* or anything similar.\n",
    "\n",
    "4.1.1 Explain how you plan to implement Linear Regression in 5-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-forth",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> Initialise the weights and bias to a zerro array and a zero respectively. </li>\n",
    "    <li> In each iteration, we calculate predicted value of y using the dot product of weights and the value of each x and adding bias. </li>\n",
    "    <li> Then, we calculate the gradient by checking the deviation from actual value of y and taking dot product with x and then normalising. </li>\n",
    "    <li> Then, we decrease weights by a factor depending on the learning rate and the gradient. </li>\n",
    "    <li> After training the model, we test it and check how good it is using measures such as RMSE. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-winter",
   "metadata": {},
   "source": [
    "4.1.2 Implement Linear Regression using `x_train` and `y_train` as the train dataset.\n",
    "\n",
    "4.1.2.1 Choose the best learning rate and print the learning rate for which you achieved the best MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01]\n",
    "num_iterations = 10000\n",
    "MSEs = []\n",
    "for rate in learning_rates:\n",
    "    num_molecules = len(x_train)\n",
    "    num_features = len(x_train[0])\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0\n",
    "    for num_iter in range(num_iterations):\n",
    "        x_arr = np.array(x_train)\n",
    "        y_pred = []\n",
    "        for x_val in x_arr:\n",
    "            predicted = np.dot(weights, x_val) + bias\n",
    "            y_pred.append(predicted)\n",
    "        y_pred_arr = np.array(y_pred)\n",
    "        y_arr = np.array(y_train)\n",
    "        gradient = np.dot(x_arr.T, (y_pred_arr - y_arr)) / y_arr.size\n",
    "        weights = weights - (rate * gradient)\n",
    "        bias = bias - (rate * np.sum(y_pred_arr - y_arr))\n",
    "    y_test_pred = []\n",
    "    x_test_arr = np.array(x_test)\n",
    "    for x_val in x_test_arr:\n",
    "        predicted = np.dot(weights, x_val) + bias\n",
    "        y_test_pred.append(predicted)\n",
    "    y_test_pred_arr = np.array(y_test_pred)\n",
    "    y_test_arr = np.array(y_test)\n",
    "    MSE = sum([(y_test_pred_arr[i] - y_test_arr[i])**2 for i in range(len(x_test))]) / len(x_test)\n",
    "    MSEs.append(float(MSE))\n",
    "\n",
    "# print(MSEs)\n",
    "blr = learning_rates[MSEs.index(min(MSEs))]\n",
    "print(\"Best learning rate: \", blr)\n",
    "print(\"MSE of best learning rate: \", min(MSEs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "angry-tampa",
   "metadata": {},
   "source": [
    "4.1.3 Make a [Parity Plot](https://en.wikipedia.org/wiki/Parity_plot) of your model's bandgap predictions on the test set with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions of x_test into `y_pred`\n",
    "\n",
    "y_pred = []\n",
    "x_test_arr = np.array(x_test)\n",
    "for x_val in x_test_arr:\n",
    "    predicted = np.dot(weights, x_val) + bias\n",
    "    y_pred.append(predicted)\n",
    "y_test_pred_arr = np.array(y_pred)\n",
    "\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "# print(len(y_test), len(y_pred))\n",
    "\n",
    "ax.scatter(y_test, y_pred)\n",
    "\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "ax.set_title('Parity Plot of Custom Linear Regression')\n",
    "ax.set_xlabel('Ground truth bandgap values')\n",
    "ax.set_ylabel('Predicted bandgap values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-chaos",
   "metadata": {},
   "source": [
    "### 4.2 Implement Ridge regression\n",
    "4.2.1 Explain Ridge regression briefly in 1-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-cyprus",
   "metadata": {},
   "source": [
    "In situations when the independent variables are highly correlated, ridge regression is a technique for estimating the coefficients of multiple regression models. The goal is to estimate the coefficients of a multiple regression model in such a way that the magnitude of the coefficients is as less as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-forwarding",
   "metadata": {},
   "source": [
    "4.2.2 Implement Ridge regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total. (You can use scikit-learn from this cell onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "violent-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═══════════╕\n",
      "│   Alphas │     RMSEs │\n",
      "╞══════════╪═══════════╡\n",
      "│   1e-15  │ 0.0866715 │\n",
      "├──────────┼───────────┤\n",
      "│   1e-05  │ 0.0866709 │\n",
      "├──────────┼───────────┤\n",
      "│   0.0001 │ 0.0866656 │\n",
      "├──────────┼───────────┤\n",
      "│   0.001  │ 0.0866128 │\n",
      "├──────────┼───────────┤\n",
      "│   0.01   │ 0.0860946 │\n",
      "├──────────┼───────────┤\n",
      "│   0.1    │ 0.081828  │\n",
      "├──────────┼───────────┤\n",
      "│   1      │ 0.101785  │\n",
      "├──────────┼───────────┤\n",
      "│  10      │ 0.31362   │\n",
      "├──────────┼───────────┤\n",
      "│ 100      │ 0.457101  │\n",
      "╘══════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "# you should not have imported sklearn before this point\n",
    "import sklearn.linear_model as sl_linear_model\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphas = [1e-15, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "RMSEs = []\n",
    "for alpha in alphas:\n",
    "    model = sl_linear_model.Ridge(alpha = alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    RMSE_sum = 0\n",
    "    for i in range(len(y_test)):\n",
    "        RMSE_sum += (y_test_pred[i] - y_test[i])**2\n",
    "    RMSE_sum = RMSE_sum / len(y_test)\n",
    "    RMSE = np.sqrt(RMSE_sum)\n",
    "    RMSEs.append(RMSE)\n",
    "\n",
    "headers = ['Alphas', 'RMSEs']\n",
    "table = zip(alphas, RMSEs)\n",
    "print(tabulate(table, headers = headers, tablefmt = \"fancy_grid\"))\n",
    "# plt.plot(alphas, RMSEs)\n",
    "# implement Ridge regression and make a table where you explore the effect of different values of `alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter alpha in Ridge regression is a regularisation parameter. It controls the complexity of the model. The model is punished for having high weights more severely the higher the value of alpha. The model is less penalised for having high weights the lower the value of alpha. Cross-validation is used to choose the alpha value. The extra penalty is equal to the square of the coefficients' magnitude. The majority of feature coefficients are cancelled out as alpha increases. More feature coefficients are employed when alpha decreases. Lowest RMSE encountered at alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-temperature",
   "metadata": {},
   "source": [
    "### 4.3 Implement Lasso regression\n",
    "4.3.1 Explain Lasso regression briefly in 1-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso Regression analysis approach is used to do both variable selection and regularisation in order to enhance the predictability and interpretability of the generated statistical model. L1 regularisation technique is used by the Lasso Regression algorithm. When there are more features, it is considered since feature selection is automatic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-wonder",
   "metadata": {},
   "source": [
    "4.3.2 Implement Lasso regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Lasso regression and make a table where you explore the effect of different values of `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accompanied-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═══════════╕\n",
      "│   Alphas │     RMSEs │\n",
      "╞══════════╪═══════════╡\n",
      "│   1e-15  │ 0.0866715 │\n",
      "├──────────┼───────────┤\n",
      "│   1e-05  │ 0.0866173 │\n",
      "├──────────┼───────────┤\n",
      "│   0.0001 │ 0.0861296 │\n",
      "├──────────┼───────────┤\n",
      "│   0.001  │ 0.0816123 │\n",
      "├──────────┼───────────┤\n",
      "│   0.01   │ 0.0871958 │\n",
      "├──────────┼───────────┤\n",
      "│   0.1    │ 0.492503  │\n",
      "├──────────┼───────────┤\n",
      "│  10      │ 0.492503  │\n",
      "├──────────┼───────────┤\n",
      "│ 100      │ 0.492503  │\n",
      "╘══════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-15, 0.00001, 0.0001, 0.001, 0.01, 0.1, 10, 100]\n",
    "RMSEs = []\n",
    "for alpha in alphas:\n",
    "    model = sl_linear_model.Lasso(alpha = alpha)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    RMSE_sum = 0\n",
    "    for i in range(len(y_test)):\n",
    "        RMSE_sum += (y_test_pred[i] - y_test[i])**2\n",
    "    RMSE_sum = RMSE_sum / len(y_test)\n",
    "    RMSE = np.sqrt(RMSE_sum)\n",
    "    RMSEs.append(RMSE)\n",
    "\n",
    "headers = ['Alphas', 'RMSEs']\n",
    "table = zip(alphas, RMSEs)\n",
    "print(tabulate(table, headers = headers, tablefmt = \"fancy_grid\"))\n",
    "# plt.plot(alphas, RMSEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso regression hyperparameter alpha regulates how much regularisation is to be performed to the model. The model is more regularly distributed the greater the value of alpha. The model is less regularised the lower the value of alpha. Cross-validation is used to choose the alpha value. The extra penalty is equal to the magnitude of the coefficients in absolute terms. Lowest RMSE encountered at alpha = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
